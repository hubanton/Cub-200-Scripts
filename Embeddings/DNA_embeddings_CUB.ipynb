{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is adapted from the work by Badirli et al.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "S. Badirli, Z. Akata, G. Mohler, C. Picard, and M. Dundar,  \n",
    "\"Fine-Grained Zero-Shot Learning with DNA as Side Information,\"  \n",
    "*Neural Information Processing Systems*, 2021  \n",
    "[Link to Repository](https://github.com/sbadirli/Fine-Grained-ZSL-with-DNA/tree/main/DNA%20Embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "*Only some missing imports and a better way to store the embeddings + barcodes were added*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 08:46:02.053293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 08:46:02.787875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "datapath = r'./data'\n",
    "dataset = 'CUB_DNA' # This is the same CUB data, just 6 classes lacking any DNA barcodes are removed \n",
    "x2 = sio.loadmat(os.path.join(datapath, dataset, 'CUB_DNA.mat')) \n",
    "x = sio.loadmat(os.path.join(datapath, dataset, 'Bird_DNA.mat'))  # Dataset containing all bird DNA barcodes extracted from COI gene\n",
    "us_cls = sio.loadmat(os.path.join(datapath, dataset, 'CUB_unseen_classes_sci_name.mat'))\n",
    "barcodes=x['nucleotides_aligned'][0]\n",
    "unseen_cls = us_cls['us_classes']\n",
    "bird_species=x['species'][0]\n",
    "cub_barcodes = x2['nucleotides_aligned'][0]\n",
    "c_labels = x2['species'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26409 2772\n"
     ]
    }
   ],
   "source": [
    "# Number of training samples and entire data\n",
    "N = len(barcodes)\n",
    "M = len(cub_barcodes)\n",
    "print(N, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcodes=[]\n",
    "#labels=[]\n",
    "b_species = []\n",
    "for i in range(N):\n",
    "    if len(barcodes[i][0])>0:\n",
    "        bcodes.append(barcodes[i][0])\n",
    "        b_species.append(bird_species[i][0])\n",
    "\n",
    "b_species = np.asarray(b_species)\n",
    "bcodes = np.asarray(bcodes)\n",
    "        \n",
    "# c_species = []\n",
    "# for i in range(len(cub_species)):\n",
    "#     c_species.append(cub_species[i][0][0].split('--> ')[1])  \n",
    "    \n",
    "cub_bcodes = []\n",
    "c_labels_ = []\n",
    "for i in range(len(cub_barcodes)):\n",
    "    cub_bcodes.append(cub_barcodes[i][0])\n",
    "    c_labels_.append(c_labels[i][0])\n",
    "    \n",
    "unseen_classes = []\n",
    "for i in range(len(unseen_cls)):\n",
    "    unseen_classes.append(unseen_cls[i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample barcode: A-----------------------------------------------------CTATACCTAATCTTCGGCGCCTGAGCTGGTATAGTCGGCACCGCCCTCAGCCTACTCATTCGCGCAGAACTCGGCCAACCAGGCACACTCCTAGGCGACGACCAAATCTACAACGTAATCGTTACTGCACATGCCTTTGTAATAATCTTCTTCATAGTTATACCAATCATGATTGGAGGATTCGGAAACTGACTTGTCCCACTCATAATTGGAGCCCCCGACATAGCCTTTCCACGCATAAACAACATAAGCTTCTGGCTACTCCCTCCATCCTTCCTCCTCCTACTGGCCTCCTCAACAGTAGAAGCAGGAGCCGGCACTGGATGAACTGTCTACCCCCCATTAGCTGGCAACATAGCCCATGCCGGAGCTTCAGTAGACTTGGCCATCTTCTCCTTACACTTAGCTGGAGTCTCATCCATTCTAGGAGCAATCAACTTTATCACAACCGCCATCAACATAAAACCCCCAGCCCTCTCCCAGTACCAAACACCCCTATTCGTATGATCTGTCCTCATTACCGCCGTCCTTCTATTACTTTCACTCCCAGTCCTAGCCGCTGGCATCACTATACTACTCACAGACCGAAACCTAAACACAACATTCTTTGACCCTGCCGGCGGAGGCGATCCTATCCTATACCAACACCTCTTCTGATTCTTTGGACACCCAGAAGTCTACATCTTAATCCT---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------A\n"
     ]
    }
   ],
   "source": [
    "print('Sample barcode: %s' % bcodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(c_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding unseen classes of CUB data from training set\n",
    "idx = np.in1d(b_species, unseen_classes)\n",
    "\n",
    "b_species = b_species[~idx]\n",
    "bcodes = bcodes[~idx]\n",
    "print(len(b_species), sum(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uy = np.unique(b_species)\n",
    "labels = np.zeros(len(b_species), dtype='int32')\n",
    "for i in range(len(uy)):\n",
    "    idx = b_species==uy[i]\n",
    "    labels[idx.ravel()] = i+1 # Matlab indexing\n",
    "    if idx.sum()<10:\n",
    "        print(uy[i])\n",
    "    \n",
    "print('Number of classes: %d' % len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes and their names from CUB data that has less than 10 samples\n",
    "# Just for information\n",
    "uy = np.unique(c_labels_)\n",
    "c_labels_ = np.asarray(c_labels_)\n",
    "c_labels = np.zeros(len(c_labels_), dtype='int32')\n",
    "for i in range(len(uy)):\n",
    "    idx = c_labels_==uy[i]\n",
    "    c_labels[idx] = i+1 # Matlab indexing\n",
    "    if np.sum(idx)<10:\n",
    "        print(uy[i])\n",
    "    \n",
    "print(len(np.unique(c_labels)), np.unique(c_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Converting base pairs, ACTG, into numbers: Tokenization\n",
    "\n",
    "### For BIRD data\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(bcodes)\n",
    "sequence_of_int = tokenizer.texts_to_sequences(bcodes)\n",
    "\n",
    "### For CUB data barcodes\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(cub_bcodes)\n",
    "sequence_of_int_cub = tokenizer.texts_to_sequences(cub_bcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bird data barcode lengths have a high variance and not as in INSECT data majority has seq length of 658\n",
    "# These barcodes are aligned ones as in INSECT data\n",
    "cnt = 0\n",
    "ll = np.zeros((len(bcodes), 1))\n",
    "for i in range(len(bcodes)):\n",
    "    ll[i] = len(sequence_of_int[i])\n",
    "    if ll[i]==658:\n",
    "        #print(i)\n",
    "        cnt += 1\n",
    "print(cnt, np.median(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculating sequence count for each species\n",
    "cl = len(np.unique(b_species))\n",
    "N = len(b_species)\n",
    "seq_len=np.zeros((cl))\n",
    "seq_cnt=np.zeros((cl))\n",
    "for i in range(N):\n",
    "    k=labels[i]-1\n",
    "    seq_cnt[k]+=1\n",
    "    #seq_len[k]+=len(sequence_of_int[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# classes: %d' % cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the all CUB DNA barcodes into one-hot encoding. Note that this data is not used during training. \n",
    "# We are getting it ready for the prediction time to get the final DNA embeddings after training is done\n",
    "\n",
    "sl = 1500 # Fixed to the max sequnce length. Note that, changing it to median value, 1548, does have an infitesimal effect\n",
    "N_cub = len(cub_bcodes)\n",
    "allX=np.zeros((N_cub,sl,5))\n",
    "for i in range(N_cub):\n",
    "    Nt=len(sequence_of_int_cub[i])\n",
    "\n",
    "    for j in range(sl):\n",
    "        if(len(sequence_of_int_cub[i])>j):\n",
    "            k=sequence_of_int_cub[i][j]-1\n",
    "            if(k>4):\n",
    "                k=4\n",
    "            allX[i][j][k]=1.0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allX.shape)\n",
    "\n",
    "print(len(cub_bcodes))\n",
    "print(c_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training matrix and labels\n",
    "trainX=np.zeros((N,sl,5))\n",
    "trainY=np.zeros((N,cl))\n",
    "labelY=np.zeros(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we cap the class sample size to 50 for a balanced training set\n",
    "\n",
    "Nc=-1\n",
    "clas_cnt=np.zeros((cl))\n",
    "for i in range(N):\n",
    "    Nt=len(sequence_of_int[i])\n",
    "    k=labels[i]-1\n",
    "    clas_cnt[k]+=1\n",
    "    itl=i+1\n",
    "    if(seq_cnt[k]>=10 and clas_cnt[k]<=50):\n",
    "        Nc=Nc+1\n",
    "        for j in range(sl):\n",
    "            if(len(sequence_of_int[i])>j):\n",
    "                k=sequence_of_int[i][j]-1\n",
    "                if(k>4):\n",
    "                    k=4\n",
    "                trainX[Nc][j][k]=1.0\n",
    "            \n",
    "        k=labels[i]-1\n",
    "        trainY[Nc][k]=1.0\n",
    "        labelY[Nc]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training size: %d' % Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the balanced training set\n",
    "trainX=trainX[0:Nc]\n",
    "trainY=trainY[0:Nc]\n",
    "labelY=labelY[0:Nc]\n",
    "print(trainX.shape, trainY.shape, labelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argwhere(np.all(trainY[..., :] == 0, axis=0))\n",
    "trainY = np.delete(trainY, idx, axis=1)\n",
    "\n",
    "print(len(idx), min(np.sum(trainY,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainY.shape)\n",
    "print(labelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the training set shape for CNN \n",
    "trainX=np.expand_dims(trainX, axis=3)\n",
    "allX=np.expand_dims(allX, axis=3)\n",
    "print(trainX.shape, allX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the training set into train and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainX, trainY, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu', input_shape=(sl, 5,1),padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "# model.add(layers.Conv2D(16, (3,3), activation='relu',padding=\"SAME\"))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(400, activation='tanh'))\n",
    "model.add(layers.Dense(y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-decay learning rate scheduler\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.001\n",
    "   drop = 0.5\n",
    "   epochs_drop = 2.0\n",
    "   lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "class LossHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))\n",
    "        \n",
    "loss_history = LossHistory()\n",
    "lrate = callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "#opt = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "# Validation time\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size = 32, validation_data=(X_test, y_test),\n",
    "                      callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "# Final Test time\n",
    "#history = model.fit(trainX, trainY, epochs=5, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Learning rates through epochs:', loss_history.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving and Loading model\n",
    "model.save('CUB_DNA_5e_adam_aligned')\n",
    "model = tf.keras.models.load_model('CUB_DNA_5e_adam_aligned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the DNA embeddings of all data from the last dense layer\n",
    "layer_name = 'dense'\n",
    "model2= models.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "dna_embeddings=model2.predict(allX)\n",
    "print(dna_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE THE RAW BARCODES AS CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('cub_dna_barcodes.csv', \"w\") as file:\n",
    "    # Write the header to the file\n",
    "    file.write(\"species, BARCODE\\n\")\n",
    "\n",
    "    # Write each line with species and corresponding DNA barcode\n",
    "    for label, sequence in zip(c_labels, cub_dna_sequences):\n",
    "        file.write(f\"{label[0]}, {sequence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE THE EMBEDDINGS AS A CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['species'] + [f'Neuron_{i}' for i in range(1, 401)]\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Populate DataFrame\n",
    "df['species'] = c_labels_\n",
    "df[column_names[1:]] = [row.tolist() for row in dna_embeddings] \n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('cub_dna_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
