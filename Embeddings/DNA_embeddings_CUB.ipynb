{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is adapted from the work by Badirli et al.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "S. Badirli, Z. Akata, G. Mohler, C. Picard, and M. Dundar,  \n",
    "\"Fine-Grained Zero-Shot Learning with DNA as Side Information,\"  \n",
    "*Neural Information Processing Systems*, 2021  \n",
    "[Link to Repository](https://github.com/sbadirli/Fine-Grained-ZSL-with-DNA/tree/main/DNA%20Embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "*Only some missing imports and a better way to store the embeddings were added*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 18:39:30.965003: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 18:39:32.219090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/CUB_DNA/CUB_DNA.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/CUB_DNA/CUB_DNA.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m datapath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUB_DNA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# This is the same CUB data, just 6 classes lacking any DNA barcodes are removed \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[43msio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCUB_DNA.mat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m sio\u001b[38;5;241m.\u001b[39mloadmat(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(datapath, dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBird_DNA.mat\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Dataset containing all bird DNA barcodes extracted from COI gene\u001b[39;00m\n\u001b[1;32m      9\u001b[0m us_cls \u001b[38;5;241m=\u001b[39m sio\u001b[38;5;241m.\u001b[39mloadmat(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(datapath, dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUB_unseen_classes_sci_name.mat\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/CUB_DNA/CUB_DNA.mat'"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "datapath = r'./data'\n",
    "dataset = 'CUB_DNA' # This is the same CUB data, just 6 classes lacking any DNA barcodes are removed \n",
    "x2 = sio.loadmat(os.path.join(datapath, dataset, 'CUB_DNA.mat')) \n",
    "x = sio.loadmat(os.path.join(datapath, dataset, 'Bird_DNA.mat'))  # Dataset containing all bird DNA barcodes extracted from COI gene\n",
    "us_cls = sio.loadmat(os.path.join(datapath, dataset, 'CUB_unseen_classes_sci_name.mat'))\n",
    "barcodes=x['nucleotides_aligned'][0]\n",
    "unseen_cls = us_cls['us_classes']\n",
    "bird_species=x['species'][0]\n",
    "cub_barcodes = x2['nucleotides_aligned'][0]\n",
    "c_labels = x2['species'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training samples and entire data\n",
    "N = len(barcodes)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcodes=[]\n",
    "#labels=[]\n",
    "b_species = []\n",
    "for i in range(N):\n",
    "    if len(barcodes[i][0])>0:\n",
    "        bcodes.append(barcodes[i][0])\n",
    "        b_species.append(bird_species[i][0])\n",
    "\n",
    "b_species = np.asarray(b_species)\n",
    "bcodes = np.asarray(bcodes)\n",
    "        \n",
    "# c_species = []\n",
    "# for i in range(len(cub_species)):\n",
    "#     c_species.append(cub_species[i][0][0].split('--> ')[1])  \n",
    "    \n",
    "cub_bcodes = []\n",
    "c_labels_ = []\n",
    "for i in range(len(cub_barcodes)):\n",
    "    cub_bcodes.append(cub_barcodes[i][0])\n",
    "    c_labels_.append(c_labels[i][0])\n",
    "    \n",
    "unseen_classes = []\n",
    "for i in range(len(unseen_cls)):\n",
    "    unseen_classes.append(unseen_cls[i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample barcode: %s' % bcodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(c_labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding unseen classes of CUB data from training set\n",
    "idx = np.in1d(b_species, unseen_classes)\n",
    "\n",
    "b_species = b_species[~idx]\n",
    "bcodes = bcodes[~idx]\n",
    "print(len(b_species), sum(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uy = np.unique(b_species)\n",
    "labels = np.zeros(len(b_species), dtype='int32')\n",
    "for i in range(len(uy)):\n",
    "    idx = b_species==uy[i]\n",
    "    labels[idx.ravel()] = i+1 # Matlab indexing\n",
    "    if idx.sum()<10:\n",
    "        print(uy[i])\n",
    "    \n",
    "print('Number of classes: %d' % len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes and their names from CUB data that has less than 10 samples\n",
    "# Just for information\n",
    "uy = np.unique(c_labels_)\n",
    "c_labels_ = np.asarray(c_labels_)\n",
    "c_labels = np.zeros(len(c_labels_), dtype='int32')\n",
    "for i in range(len(uy)):\n",
    "    idx = c_labels_==uy[i]\n",
    "    c_labels[idx] = i+1 # Matlab indexing\n",
    "    if np.sum(idx)<10:\n",
    "        print(uy[i])\n",
    "    \n",
    "print(len(np.unique(c_labels)), np.unique(c_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Converting base pairs, ACTG, into numbers: Tokenization\n",
    "\n",
    "### For BIRD data\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(bcodes)\n",
    "sequence_of_int = tokenizer.texts_to_sequences(bcodes)\n",
    "\n",
    "### For CUB data barcodes\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(cub_bcodes)\n",
    "sequence_of_int_cub = tokenizer.texts_to_sequences(cub_bcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bird data barcode lengths have a high variance and not as in INSECT data majority has seq length of 658\n",
    "# These barcodes are aligned ones as in INSECT data\n",
    "cnt = 0\n",
    "ll = np.zeros((len(bcodes), 1))\n",
    "for i in range(len(bcodes)):\n",
    "    ll[i] = len(sequence_of_int[i])\n",
    "    if ll[i]==658:\n",
    "        #print(i)\n",
    "        cnt += 1\n",
    "print(cnt, np.median(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculating sequence count for each species\n",
    "cl = len(np.unique(b_species))\n",
    "N = len(b_species)\n",
    "seq_len=np.zeros((cl))\n",
    "seq_cnt=np.zeros((cl))\n",
    "for i in range(N):\n",
    "    k=labels[i]-1\n",
    "    seq_cnt[k]+=1\n",
    "    #seq_len[k]+=len(sequence_of_int[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# classes: %d' % cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the all CUB DNA barcodes into one-hot encoding. Note that this data is not used during training. \n",
    "# We are getting it ready for the prediction time to get the final DNA embeddings after training is done\n",
    "\n",
    "sl = 1500 # Fixed to the max sequnce length. Note that, changing it to median value, 1548, does have an infitesimal effect\n",
    "N_cub = len(cub_bcodes)\n",
    "allX=np.zeros((N_cub,sl,5))\n",
    "for i in range(N_cub):\n",
    "    Nt=len(sequence_of_int_cub[i])\n",
    "\n",
    "    for j in range(sl):\n",
    "        if(len(sequence_of_int_cub[i])>j):\n",
    "            k=sequence_of_int_cub[i][j]-1\n",
    "            if(k>4):\n",
    "                k=4\n",
    "            allX[i][j][k]=1.0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allX.shape)\n",
    "\n",
    "print(len(cub_bcodes))\n",
    "print(c_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training matrix and labels\n",
    "trainX=np.zeros((N,sl,5))\n",
    "trainY=np.zeros((N,cl))\n",
    "labelY=np.zeros(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we cap the class sample size to 50 for a balanced training set\n",
    "\n",
    "Nc=-1\n",
    "clas_cnt=np.zeros((cl))\n",
    "for i in range(N):\n",
    "    Nt=len(sequence_of_int[i])\n",
    "    k=labels[i]-1\n",
    "    clas_cnt[k]+=1\n",
    "    itl=i+1\n",
    "    if(seq_cnt[k]>=10 and clas_cnt[k]<=50):\n",
    "        Nc=Nc+1\n",
    "        for j in range(sl):\n",
    "            if(len(sequence_of_int[i])>j):\n",
    "                k=sequence_of_int[i][j]-1\n",
    "                if(k>4):\n",
    "                    k=4\n",
    "                trainX[Nc][j][k]=1.0\n",
    "            \n",
    "        k=labels[i]-1\n",
    "        trainY[Nc][k]=1.0\n",
    "        labelY[Nc]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training size: %d' % Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the balanced training set\n",
    "trainX=trainX[0:Nc]\n",
    "trainY=trainY[0:Nc]\n",
    "labelY=labelY[0:Nc]\n",
    "print(trainX.shape, trainY.shape, labelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argwhere(np.all(trainY[..., :] == 0, axis=0))\n",
    "trainY = np.delete(trainY, idx, axis=1)\n",
    "\n",
    "print(len(idx), min(np.sum(trainY,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainY.shape)\n",
    "print(labelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the training set shape for CNN \n",
    "trainX=np.expand_dims(trainX, axis=3)\n",
    "allX=np.expand_dims(allX, axis=3)\n",
    "print(trainX.shape, allX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the training set into train and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainX, trainY, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu', input_shape=(sl, 5,1),padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Conv2D(16, (3,3), activation='relu',padding=\"SAME\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((3,1)))\n",
    "# model.add(layers.Conv2D(16, (3,3), activation='relu',padding=\"SAME\"))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((3,1)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(400, activation='tanh'))\n",
    "model.add(layers.Dense(y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-decay learning rate scheduler\n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.001\n",
    "   drop = 0.5\n",
    "   epochs_drop = 2.0\n",
    "   lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "class LossHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))\n",
    "        \n",
    "loss_history = LossHistory()\n",
    "lrate = callbacks.LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "#opt = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "# Validation time\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size = 32, validation_data=(X_test, y_test),\n",
    "                      callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "# Final Test time\n",
    "#history = model.fit(trainX, trainY, epochs=5, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Learning rates through epochs:', loss_history.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving and Loading model\n",
    "model.save('CUB_DNA_5e_adam_aligned')\n",
    "model = tf.keras.models.load_model('CUB_DNA_5e_adam_aligned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the DNA embeddings of all data from the last dense layer\n",
    "layer_name = 'dense'\n",
    "model2= models.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "dna_embeddings=model2.predict(allX)\n",
    "print(dna_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dna_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['species'] + [f'Neuron_{i}' for i in range(1, 401)]\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Populate DataFrame\n",
    "df['species'] = c_labels_\n",
    "df[column_names[1:]] = [row.tolist() for row in dna_embeddings] \n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('cub_200_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
